/*
 *  SPDX-License-Identifier: Apache-2.0
 *
 *  Copyright The original authors
 *
 *  Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
 */
package dev.morling.hardwood.internal.reader;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.util.ArrayList;
import java.util.List;

import dev.morling.hardwood.internal.compression.Decompressor;
import dev.morling.hardwood.internal.compression.DecompressorFactory;
import dev.morling.hardwood.internal.thrift.PageHeaderReader;
import dev.morling.hardwood.internal.thrift.ThriftCompactReader;
import dev.morling.hardwood.metadata.ColumnChunk;
import dev.morling.hardwood.metadata.ColumnMetaData;
import dev.morling.hardwood.metadata.CompressionCodec;
import dev.morling.hardwood.metadata.PageHeader;
import dev.morling.hardwood.schema.ColumnSchema;

/**
 * Scans page boundaries in a single column chunk and creates PageInfo objects.
 * <p>
 * Reads page headers and parses dictionary pages upfront, then creates
 * PageInfo records that can be used for on-demand page decoding.
 * </p>
 */
public class PageScanner {

    private final FileChannel channel;
    private final ColumnSchema columnSchema;
    private final ColumnChunk columnChunk;

    public PageScanner(FileChannel channel, ColumnSchema columnSchema, ColumnChunk columnChunk) {
        this.channel = channel;
        this.columnSchema = columnSchema;
        this.columnChunk = columnChunk;
    }

    /**
     * Scan pages in this column chunk and return PageInfo objects.
     * <p>
     * Dictionary pages are parsed upfront and the parsed dictionary is shared
     * with all data page PageInfo objects. Each PageInfo receives a ByteBuffer
     * slice of the pre-mapped chunk, avoiding per-page memory mapping overhead.
     * </p>
     *
     * @return list of PageInfo objects for data pages in this chunk
     */
    public List<PageInfo> scanPages() throws IOException {
        ColumnMetaData metaData = columnChunk.metaData();

        Long dictOffset = metaData.dictionaryPageOffset();
        long chunkStartOffset = (dictOffset != null && dictOffset > 0)
                ? dictOffset
                : metaData.dataPageOffset();
        long chunkSize = metaData.totalCompressedSize();

        MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_ONLY, chunkStartOffset, chunkSize);

        List<PageInfo> pageInfos = new ArrayList<>();
        long valuesRead = 0;
        int position = 0;

        Dictionary dictionary = null;

        while (valuesRead < metaData.numValues() && position < buffer.limit()) {
            PageReader.ByteBufferInputStream headerStream =
                new PageReader.ByteBufferInputStream(buffer, position);
            ThriftCompactReader headerReader = new ThriftCompactReader(headerStream);
            PageHeader header = PageHeaderReader.read(headerReader);
            int headerSize = headerStream.getBytesRead();

            int pageDataOffset = position + headerSize;
            int compressedSize = header.compressedPageSize();
            int totalPageSize = headerSize + compressedSize;

            if (header.type() == PageHeader.PageType.DICTIONARY_PAGE) {
                byte[] compressedData = new byte[compressedSize];
                buffer.slice(pageDataOffset, compressedSize).get(compressedData);
                int numValues = header.dictionaryPageHeader().numValues();
                int uncompressedSize = header.uncompressedPageSize();

                dictionary = parseDictionary(compressedData, numValues, uncompressedSize,
                    columnSchema, metaData.codec());
            }
            else if (header.type() == PageHeader.PageType.DATA_PAGE ||
                     header.type() == PageHeader.PageType.DATA_PAGE_V2) {
                ByteBuffer pageSlice = buffer.slice(position, totalPageSize);

                PageInfo pageInfo = new PageInfo(
                    pageSlice,
                    columnSchema,
                    metaData,
                    dictionary
                );
                pageInfos.add(pageInfo);

                valuesRead += getValueCount(header);
            }

            position += totalPageSize;
        }

        return pageInfos;
    }

    private Dictionary parseDictionary(byte[] compressedData, int numValues,
            int uncompressedSize, ColumnSchema column, CompressionCodec codec) throws IOException {
        Decompressor decompressor = DecompressorFactory.getDecompressor(codec);
        byte[] data = decompressor.decompress(compressedData, uncompressedSize);
        return Dictionary.parse(data, numValues, column.type(), column.typeLength());
    }

    private long getValueCount(PageHeader header) {
        return switch (header.type()) {
            case DATA_PAGE -> header.dataPageHeader().numValues();
            case DATA_PAGE_V2 -> header.dataPageHeaderV2().numValues();
            case DICTIONARY_PAGE -> 0;
            case INDEX_PAGE -> 0;
        };
    }
}
